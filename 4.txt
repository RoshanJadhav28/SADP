q1]

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Load the dataset
data = pd.read_csv('Mall_Customers (1).csv')

# Display the first few rows of the dataset to understand its structure
print(data.head())

# Checking for columns and data types
print(data.info())

# Assuming the columns are 'CustomerID', 'Genre', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)'
# Select relevant features for clustering, e.g., 'Annual Income (k$)' and 'Spending Score (1-100)'
X = data[['Annual Income (k$)', 'Spending Score (1-100)']]

# Standardize the features to normalize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Use the elbow method to determine the optimal number of clusters
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# Plot the elbow graph
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS (Within-Cluster Sum of Square)')
plt.show()

# Based on the elbow plot, we choose the number of clusters (e.g., 5)
k = 5
kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)
y_pred = kmeans.fit_predict(X_scaled)

# Add the cluster labels to the original dataset
data['Cluster'] = y_pred

# Visualize the clusters
plt.figure(figsize=(10, 6))
plt.scatter(X_scaled[y_pred == 0,0],X_scaled[y_pred == 0,1],s=50,c='red',label='cluster1')
plt.scatter(X_scaled[y_pred == 1,0],X_scaled[y_pred == 1,1],s=50,c='green',label='cluster2')
plt.scatter(X_scaled[y_pred == 2,0],X_scaled[y_pred == 2,1],s=50,c='yellow',label='cluster3')
plt.scatter(X_scaled[y_pred == 3,0],X_scaled[y_pred == 3,1],s=50,c='blue',label='cluster4')
plt.scatter(X_scaled[y_pred == 4,0],X_scaled[y_pred == 4,1],s=50,c='black',label='cluster5')
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1],s=80,c='magenta',label='centroid')
plt.title('Clusters of Mall Customers')
plt.xlabel('Annual Income (scaled)')
plt.ylabel('Spending Score (scaled)')
plt.legend()
plt.show()
___________________________________________________
q2]

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset
data = pd.read_csv("HousePricePrediction.csv")

# Remove rows with any null values
data_cleaned = data.dropna()

# Select the feature and target variable
X = data_cleaned[['LotArea']]
y = data_cleaned['SalePrice']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Initialize and train the Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

# Print results
print("Mean Squared Error:", rmse)
print("RÂ² Score:", r2)

plt.scatter(X_train, y_train, color='blue', label='Training Data')
plt.scatter(X_test, y_test, color='green', label='Test Data')
plt.plot(X_test, y_pred, color='red', label='Regression Line')
plt.xlabel('Size (square feet)')
plt.ylabel('Price (in $)')
plt.title('Simple Linear Regression - House Price Prediction')
plt.legend()
plt.show()

