q1]

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data  # Features
y = iris.target  # Labels

print(X)
print(y)

# Display the shape of the dataset
print("Original shape:", X.shape)

# Apply PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
print(X_pca)

# Display the new shape
print("Reduced shape:", X_pca.shape)

# Visualize the PCA results
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolor='k', cmap='viridis', s=100)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of Iris Dataset')
plt.colorbar()
plt.show()

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)
# Initialize and train the model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)

# Calculate and display the accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

# New flower measurements
new_flower = np.array([[5.1, 3.5, 1.4, 0.2]])

# Transform the new data using the same PCA
new_flower_pca = pca.transform(new_flower)
# Predict the class of the new flower
predicted_class = model.predict(new_flower_pca)
predicted_species = iris.target_names[predicted_class][0]
print(f'Predicted species for the new flower: {predicted_species}')
_____________________________________________________
q2]

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

df=pd.read_csv('Employee_Salary_Dataset.csv')
print(df.head())

#converting the categorial data into numerical 0-'female' and 1-'Male'
le=LabelEncoder()

df['Gender']=le.fit_transform(df['Gender'])
print(df.head())

# Standardizing the data (K-means performs better with scaled data)
scaler = StandardScaler()
scaled_df = scaler.fit_transform(df)

# Finding the optimal value of k using Elbow Method
sse = []
k_range = range(1, 11)  # Trying k from 1 to 10
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(scaled_df)
    sse.append(kmeans.inertia_)  # Sum of squared distances to centroids

# Plotting the Elbow Curve
plt.figure(figsize=(8, 6))
plt.plot(k_range, sse, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Sum of squared distances (Inertia)')
plt.show()

# Using Silhouette Score to validate the number of clusters
silhouette_scores = []
for k in range(2, 11):  # Starting from 2 clusters
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(scaled_df)
    cluster_labels = kmeans.labels_
    silhouette_avg = silhouette_score(scaled_df, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Plotting Silhouette Scores
plt.figure(figsize=(8, 6))
plt.plot(range(2, 11), silhouette_scores, marker='o')
plt.title('Silhouette Scores for k')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette Score')
plt.show()

# Based on the elbow method and silhouette score, let's assume we choose k=3
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
df['Cluster'] = kmeans.fit_predict(scaled_df)

# Showing the clustered data
print(df.head(15))

#visualize the clusters (for 2D data)
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Salary', y='Experience_Years', hue='Cluster', data=df, palette='viridis')
plt.title('Employee Clusters based on Salary and Experience_Years')
plt.show()

