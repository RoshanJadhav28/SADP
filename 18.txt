q1]

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_diabetes
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Load the Diabetes dataset
# (If you have your own diabetes dataset in CSV format, replace this with pd.read_csv('your_file.csv'))
diabetes_data = load_diabetes(as_frame=True)
data = diabetes_data.data

# y=print(diabetes_data.target)
# Standardize the data
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)

# Determine the optimal number of clusters using the Elbow Method
inertia = []
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data_scaled)
    inertia.append(kmeans.inertia_)

# Plot the Elbow curve
plt.figure(figsize=(8, 5))
plt.plot(range(1, 10), inertia, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal K')
plt.show()

# Apply K-means with the chosen number of clusters (e.g., 3 clusters based on the Elbow Method)
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(data_scaled)

# Add the cluster assignments to the original data
data['Cluster'] = clusters

'''# Optional: Visualize the clusters in a 2D plot using PCA for dimensionality reduction
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data_scaled)
plt.figure(figsize=(8, 5))
plt.scatter(data_pca[:, 0], data_pca[:, 1], c=clusters, cmap='viridis')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.title('K-means Clusters (after PCA)')
plt.colorbar(label='Cluster')
plt.show()
'''
# Print out the first few rows of the dataset with cluster labels
print(data.head())
_____________________________________________________________________________________
q2]

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error,r2_score

#load the dataset
df=pd.read_csv("Position_Salaries.csv")
print(df)

#preapre the data
X=df[['Level']].values
y=df['Salary'].values

#polynomial 
poly=PolynomialFeatures(degree=4) 
X_poly=poly.fit_transform(X) 

poly_model=LinearRegression()
poly_model.fit(X_poly,y)

y_pred_poly=poly_model.predict(X_poly)

#plot polynomial regression
plt.scatter(X,y,color='green')
plt.plot(X,y_pred_poly,color='yellow')
plt.title('Polynomial linear regression')
plt.xlabel("Level")
plt.ylabel("Salary")
plt.show()

#evaluate the polynomial model
mse_poly=mean_squared_error(y,y_pred_poly)
r2_poly=r2_score(y,y_pred_poly)

print(f"Mean squared error :{mse_poly}")
print(f"R2 score:{r2_poly}")

#predict the salary for level 11 and 12 polynomial linear
level_11_12=poly.transform([[11],[12]])
salary_pred_poly=poly_model.predict(level_11_12)
print(f"polynomial regression")
print(f"Predicted salary for level 11:{salary_pred_poly[0]}")
print(f"Predicted salary for level 12:{salary_pred_poly[1]}")

